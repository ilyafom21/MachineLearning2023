{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Домашнее задание 2 - предобработка признаков, pandas\n\n\n### О задании\n\nПрактическое задание 2 посвящено изучению основных библиотек для анализа данных, а также линейных моделей и методов их обучения. Вы научитесь:\n * применять библиотеки NumPy и Pandas для осуществления желаемых преобразований;\n * подготавливать данные для обучения линейных моделей;\n * обучать линейную, Lasso и Ridge-регрессии при помощи модуля scikit-learn;\n * реализовывать обычный и стохастический градиентные спуски;\n * обучать линейную регрессию для произвольного функционала качества.\n \n\n### Оценивание и штрафы\n\nКаждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов. Кроме того, некоторые из заданий являются опциональными (необязательными), однако за их выполнение можно получить дополнительные баллы, которые позднее будут учитываться при проставлении оценок автоматом по курсу.\n\nСдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n\nЗадание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце Вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник). \n\nНеэффективная реализация кода может негативно отразиться на оценке.\n\n\n### Формат сдачи\nДля сдачи задания получившийся файл \\*.ipynb с решением необходимо выложить в свой репозиторий github.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Библиотеки для анализа данных\n\n### NumPy\n\nВо всех заданиях данного раздела запрещено использовать циклы  и list comprehensions. Под вектором и матрицей в данных заданиях понимается одномерный и двумерный numpy.array соответственно.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np",
      "metadata": {},
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**1. (0.2 балла)** Реализуйте функцию, возвращающую максимальный элемент в векторе x среди элементов, перед которыми стоит нулевой. Для x = np.array([6, 2, 0, 3, 0, 0, 5, 7, 0]) ответом является 5. Если нулевых элементов нет, функция должна возвращать None.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n   import numpy as np\n\ndef max_after_zero(x):\n    # Находим индексы нулевых элементов в массиве x\n    indices = np.where(x == 0)[0]\n    # Если нет ни одного нулевого элемента - возвращаем None\n    if len(indices) == 0:\n        return None\n    # Ищем индексы элементов, следующих за нулевыми элементами\n    next_indices = indices + 1\n    # Отбираем только валидные индексы\n    valid_indices = next_indices[next_indices < len(x)]\n    # Если нет ни одного валидного индекса - возвращаем None\n    if len(valid_indices) == 0:\n        return None\n    # Находим максимальный элемент среди валидных индексов\n    max_value = np.max(x[valid_indices])\n    return max_value\n\nx = np.array([6, 2, 0, 3, 0, 0, 5, 7, 0])\nresult = max_after_zero(x)\nprint(result)\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**2. (0.2 балла)** Реализуйте функцию, принимающую на вход матрицу и некоторое число и возвращающую ближайший к числу элемент матрицы. Например: для X = np.arange(0,10).reshape((2, 5)) и v = 3.6 ответом будет 4.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\ndef find_closest_element(matrix, num):\n    abs_diff = np.abs(matrix - num)  # вычисляем абсолютные разности\n    min_index = np.argmin(abs_diff)  # находим индекс элемента с минимальной разностью\n    closest_element = matrix.flat[min_index]  # получаем ближайший элемент\n    return closest_element\n\nX = np.arange(0, 10).reshape((2, 5))\nv = 3.6\nresult = find_closest_element(X, v)\nprint(result)",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**3. (0.2 балла)** Реализуйте функцию scale(X), которая принимает на вход матрицу и масштабирует каждый ее столбец (вычитает выборочное среднее и делит на стандартное отклонение). Убедитесь, что в функции не будет происходить деления на ноль. Протестируйте на случайной матрице (для её генерации можно использовать, например, функцию [numpy.random.randint](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html)).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\ndef scale(X):\n    scaled_matrix = (X - np.mean(X, axis=0)) / np.std(X, axis=0, ddof=1)\n    return scaled_matrix\n\n# Тестирование\nrandom_matrix = np.random.randint(0, 10, size=(3, 4))\nscaled_matrix = scale(random_matrix)\nprint(\"Original matrix:\")\nprint(random_matrix)\nprint(\"Scaled matrix:\")\nprint(scaled_matrix)\n",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**4. (0.2 балла)** Реализуйте функцию, которая для заданной матрицы находит:\n - определитель\n - след\n - наименьший и наибольший элементы\n - норму Фробениуса\n - собственные числа\n - обратную матрицу\n\nДля тестирования сгенерируйте матрицу с элементами из нормального распределения $\\mathcal{N}$(10,1)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\nimport numpy as np\n\ndef matrix_operations(matrix):\n    # Определитель\n    determinant = np.linalg.det(matrix)\n    print(\"Определитель: \", determinant)\n    \n    # След\n    trace = np.trace(matrix)\n    print(\"След: \", trace)\n    \n    # Наименьший и наибольший элементы\n    min_element = np.min(matrix)\n    max_element = np.max(matrix)\n    print(\"Наименьший элемент: \", min_element)\n    print(\"Наибольший элемент: \", max_element)\n    \n    # Норма Фробениуса\n    frobenius_norm = np.linalg.norm(matrix, 'fro')\n    print(\"Норма Фробениуса: \", frobenius_norm)\n    \n    # Собственные числа\n    eigenvalues = np.linalg.eigvals(matrix)\n    print(\"Собственные числа: \", eigenvalues)\n    \n    # Обратная матрица\n    inverse_matrix = np.linalg.inv(matrix)\n    print(\"Обратная матрица: \", inverse_matrix)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**5. (0.2 балла)** Повторите 100 раз следующий эксперимент: сгенерируйте две матрицы размера 10×10 из стандартного нормального распределения, перемножьте их (как матрицы) и найдите максимальный элемент. Какое среднее значение по экспериментам у максимальных элементов? 95-процентная квантиль?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\nmax_values = []\n\nfor _ in range(100):\n    matrix1 = np.random.standard_normal((10, 10))\n    matrix2 = np.random.standard_normal((10, 10))\n    product = np.matmul(matrix1, matrix2)\n    max_value = np.max(product)\n    max_values.append(max_value)\n\nmean_max_value = np.mean(max_values)\nquantile_95 = np.percentile(max_values, 95)\n",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Pandas\n\n![](https://metrouk2.files.wordpress.com/2015/10/panda.jpg)\n\n#### Ответьте на вопросы о данных по авиарейсам в США за январь-апрель 2008 года.\n\nДанные находятся в приложенном файле `2008.csv`. Их [описание](http://stat-computing.org/dataexpo/2009/the-data.html) приведено ниже:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Airline on-time performance\n\nHave you ever been stuck in an airport because your flight was delayed or cancelled and wondered if you could have predicted it if you'd had more data? This is your chance to find out.\n\nThe data\nThe data set is available for download here.\nThe data consists of flight arrival and departure details for all commercial flights within the USA, from October 1987 to April 2008. This is a large dataset: there are nearly 120 million records in total, and takes up 1.6 gigabytes of space compressed and 12 gigabytes when uncompressed.\n\nUnderstanding and preparing the data\nIn order to answer above questions, we are going to analyze the provided dataset, containing up to 1936758 ### different internal flights in the US for 2008 and their causes for delay, diversion and cancellation\n\nThe data comes from the U.S. Department of Transportation’s (DOT) Bureau of Transportation Statistics (BTS). Meta data explanations\n\nThis dataset is composed by the following variables:\n\n**Year** 2008 **Month** 1-12 **DayofMonth** 1-31 **DayOfWeek** 1 (Monday) - 7 (Sunday)  \n**DepTime** actual departure time (local, hhmm)  \n**CRSDepTime** scheduled departure time (local, hhmm)  \n**ArrTime** actual arrival time (local, hhmm)  \n**CRSArrTime** scheduled arrival time (local, hhmm)  \n**UniqueCarrier** unique carrier code  \n**FlightNum** flight number  \n**TailNum** plane tail number: aircraft registration, unique aircraft identifier  \n**ActualElapsedTime** in minutes  \n**CRSElapsedTime** in minutes  \n**AirTime** in minutes  \n**ArrDelay** arrival delay, in minutes: A flight is counted as “on time” if it operated less than 15 minutes later the scheduled time shown in the carriers’ Computerized Reservations Systems (CRS).  \n**DepDelay** departure delay, in minutes  \n**Origin** origin IATA airport code  \n**Dest** destination IATA airport code  \n**Distance** in miles  \n**TaxiIn** taxi in time, in minutes  \n**TaxiOut** taxi out time in minutes  \n**Cancelled** *was the flight cancelled  \n**CancellationCode** reason for cancellation (A = carrier, B = weather, C = NAS, D = security)  \n**Diverted** 1 = yes, 0 = no  \n**CarrierDelay** in minutes: Carrier delay is within the control of the air carrier. Examples of occurrences that may determine carrier delay are: aircraft cleaning, aircraft damage, awaiting the arrival of connecting passengers or crew, baggage, bird strike, cargo loading, catering, computer, outage-carrier equipment, crew legality (pilot or attendant rest), damage by hazardous goods, engineering inspection, fueling, handling disabled passengers, late crew, lavatory servicing, maintenance, oversales, potable water servicing, removal of unruly passenger, slow boarding or seating, stowing carry-on baggage, weight and balance delays.  \n**WeatherDelay** in minutes: Weather delay is caused by extreme or hazardous weather conditions that are forecasted or manifest themselves on point of departure, enroute, or on point of arrival.  \n**NASDelay** in minutes: Delay that is within the control of the National Airspace System (NAS) may include: non-extreme weather conditions, airport operations, heavy traffic volume, air traffic control, etc.  \n**SecurityDelay** in minutes: Security delay is caused by evacuation of a terminal or concourse, re-boarding of aircraft because of security breach, inoperative screening equipment and/or long lines in excess of 29 minutes at screening areas.  \n**LateAircraftDelay** in minutes: Arrival delay at an airport due to the late arrival of the same aircraft at a previous airport. The ripple effect of an earlier delay at downstream airports is referred to as delay propagation.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\n%matplotlib inline",
      "metadata": {},
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**6. (0.3 балла)** Какая из причин отмены рейса (`CancellationCode`) была самой частой? (расшифровки кодов можно найти в описании данных)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "reys['CancellationCode'].value_counts()",
      "metadata": {},
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**7. (0.3 балла)** Найдите среднее, минимальное и максимальное расстояние, пройденное самолетом.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n = reys['Distance'].min()\nx = reys['Distance'].max()\nsred = reys['Distance'].sred()\nprint(f'Среднее: {sred}, минимальное: {n}, максимальное: {x}')",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**8. (0.3 балла)** Не выглядит ли подозрительным минимальное пройденное расстояние? В какие дни и на каких рейсах оно было? Какое расстояние было пройдено этими же рейсами в другие дни?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "min_dist_reys = reys[reys['Distance'] == n]\nmin_dist_reys[['Year', 'Month', 'DayofMonth', 'FlightNum']]",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "flight_65_distances = reys[(reys['FlightNum'] == 65) | (reys['FlightNum'] == 64)][['FlightNum','Distance']].reset_index()\nflight_65_distances.sort_values(by = ['Distance','FlightNum'], ascending = False)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**9. (0.3 балла)** Из какого аэропорта было произведено больше всего вылетов? В каком городе он находится?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "reys['Origin'].value_counts()",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**10. (0.3 балла)** Найдите для каждого аэропорта среднее время полета (`AirTime`) по всем вылетевшим из него рейсам. Какой аэропорт имеет наибольшее значение этого показателя?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "reys.groupby('Origin')['AirTime'].mean().reset_index().sort_values(by = ['AirTime'], ascending = False)",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**11. (0.5 балла)** Найдите аэропорт, у которого наибольшая доля задержанных (`DepDelay > 0`) рейсов. Исключите при этом из рассмотрения аэропорты, из которых было отправлено меньше 1000 рейсов (используйте функцию `filter` после `groupby`).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "grouped = reys.groupby('Origin').agg({'FlightNum': 'count', 'DepDelay': lambda x: (x > 0).sum()})\nfiltered = grouped[grouped['FlightNum'] >= 1000]\nfiltered['DelayRatio'] = filtered['DepDelay'] / filtered['FlightNum']\n# airport_with_max_delay_ratio = filtered['DelayRatio'].idxmax()\nprint(filtered.sort_values(by = ['DelayRatio'], ascending = False))\nprint(\"Аэропорт с наибольшей долей задержанных рейсов: \", airport_with_max_delay_ratio)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Линейная регрессия\n\nВ этой части мы разберемся с линейной регрессией, способами её обучения и измерением качества ее прогнозов. \n\nБудем рассматривать датасет из предыдущей части задания для предсказания времени задержки отправления рейса в минутах (DepDelay). Отметим, что под задержкой подразумевается не только опоздание рейса относительно планируемого времени вылета, но и отправление до планируемого времени.\n\n### Подготовка данных\n\n**12. (0.5 балла)** Считайте выборку из файла при помощи функции pd.read_csv и ответьте на следующие вопросы:\n   - Имеются ли в данных пропущенные значения?\n   - Сколько всего пропущенных элементов в таблице \"объект-признак\"?\n   - Сколько объектов имеют хотя бы один пропуск?\n   - Сколько признаков имеют хотя бы одно пропущенное значение?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_csv(\"2008.csv\")",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.isnull().sum()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.isnull().sum().sum()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "sum(df.isnull().sum(axis=1).values != 0)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "sum(df.isnull().sum().values != 0)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Как вы понимаете, также не имеет смысла рассматривать при решении поставленной задачи объекты с пропущенным значением целевой переменной. В связи с этим ответьте на следующие вопросы и выполните соответствующие действия:\n- Имеются ли пропущенные значения в целевой переменной?\n- Проанализируйте объекты с пропущенными значениями целевой переменной. Чем вызвано это явление? Что их объединяет? Можно ли в связи с этим, на ваш взгляд, исключить какие-то признаки из рассмотрения? Обоснуйте свою точку зрения.\n\nИсключите из выборки объекты **с пропущенным значением целевой переменной и со значением целевой переменной, равным 0**, а также при необходимости исключите признаки в соответствии с вашим ответом на последний вопрос из списка и выделите целевую переменную в отдельный вектор, исключив её из матрицы \"объект-признак\".",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df['DepDelay'].isnull().sum()",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.loc[df['DepDelay'].isnull()]\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.loc[(df['DepDelay'].isnull()) & (df['Cancelled'] == 1)]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df_new = df.loc[(df['DepDelay'].isnull() != True) & (df['DepDelay'] != 0)]\ndf_new",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "DepDelay_vector = df_new['DepDelay'].values\nDepDelay_vector",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df_new = df_new.drop('DepDelay', axis=1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**13. (0.5 балла)** Обратите внимание, что признаки DepTime, CRSDepTime, ArrTime, CRSArrTime приведены в формате hhmm, в связи с чем будет не вполне корректно рассматривать их как вещественные.\n\nПреобразуйте каждый признак FeatureName из указанных в пару новых признаков FeatureName\\_Hour, FeatureName\\_Minute, разделив каждое из значений на часы и минуты. Не забудьте при этом исключить исходный признак из выборки. В случае, если значение признака отсутствует, значения двух новых признаков, его заменяющих, также должны отсутствовать. \n\nНапример, признак DepTime необходимо заменить на пару признаков DepTime_Hour, DepTime_Minute. При этом, например, значение 155 исходного признака будет преобразовано в значения 1 и 55 признаков DepTime_Hour, DepTime_Minute соответственно.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\ncolumns = [\"DepTime\", \"CRSDepTime\", \"ArrTime\", \"CRSArrTime\"]\nfor column in columns:\n    df_column = df[column]\n    hour_name = column + \"_Hour\"\n    minute_name = column + \"_Minute\"\n    df[hour_name] = df_column // 100\n    df[minute_name] = df_column % 100\n    df = df.drop(columns=[column])\ndf",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**14. (0.5 балла)** Некоторые из признаков, отличных от целевой переменной, могут оказывать чересчур значимое влияние на прогноз, поскольку по своему смыслу содержат большую долю информации о значении целевой переменной. Изучите описание датасета и исключите признаки, сильно коррелирующие с ответами. Ваш выбор признаков для исключения из выборки обоснуйте. Кроме того, исключите признаки TailNum и Year.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#airport.corr(numeric_only=True)[['DepDelay']]",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "airport = airport.drop(columns='TailNum')\nairport = airport.drop(columns='Year')\nairport = airport.drop(columns='CarrierDelay')\nairport = airport.drop(columns='WeatherDelay')\nairport = airport.drop(columns='NASDelay')\nairport = airport.drop(columns='SecurityDelay')\nairport = airport.drop(columns='LateAircraftDelay')\nairport = airport.drop(columns='ArrDelay')\nairport",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**15. (1 балл)** Приведем данные к виду, пригодному для обучения линейных моделей. Для этого вещественные признаки надо отмасштабировать, а категориальные — привести к числовому виду. Также надо устранить пропуски в данных.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "В первую очередь поймем, зачем необходимо применять масштабирование. Следующие ячейки с кодом построят гистограммы для 3 вещественных признаков выборки.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df['DepTime_Hour'].hist(bins=20)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df['TaxiIn'].hist(bins=20)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df['FlightNum'].hist(bins=20)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Какую проблему вы наблюдаете на этих графиках? Как масштабирование поможет её исправить?",
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Некоторые из признаков в нашем датасете являются категориальными. Типичным подходом к работе с ними является бинарное, или [one-hot-кодирование](https://en.wikipedia.org/wiki/One-hot).\n\nРеализуйте функцию transform_data, которая принимает на вход DataFrame с признаками и выполняет следующие шаги:\n1. Замена пропущенных значений на нули для вещественных признаков и на строки 'nan' для категориальных.\n2. Масштабирование вещественных признаков с помощью [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n3. One-hot-кодирование категориальных признаков с помощью [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) или функции [pd.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html).\n\nМетод должен возвращать преобразованный DataFrame, который должна состоять из масштабированных вещественных признаков и закодированных категориальных (исходные признаки должны быть исключены из выборки).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n\n\ndef transform_data(data):\n    number_columns = []\n    str_columns = []\n\n    columns = data.columns\n    \n    for column in columns:\n        item_type = str(data[column].dtypes)\n        if \"int\" in item_type or \"float\" in item_type:\n            number_columns.append(column)\n        else:\n            str_columns.append(column)\n\n    data[number_columns] = data[number_columns].fillna(0)\n    data[str_columns] = data[str_columns].fillna(\"nan\")\n    \n    scaler = StandardScaler()\n    scaler.fit(data[number_columns])\n    data[number_columns] = scaler.transform(data[number_columns])\n    \n    data = pd.get_dummies(data)\n    \n    return data",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Примените функцию transform_data к данным. Сколько признаков получилось после преобразования?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your code here\ndf = transform_data(df)",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**16. (0.5 балла)** Разбейте выборку и вектор целевой переменной на обучение и контроль в отношении 70/30 (для этого можно использовать, например, функцию [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)). ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_train, X_test, Y_train, Y_test = train_test_split(airport, DepDelay , test_size=0.3, random_state=42)",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Scikit-learn\n\n<img src = \"https://pp.vk.me/c4534/u35727827/93547647/x_d31c4463.jpg\">\nТеперь, когда мы привели данные к пригодному виду, попробуем решить задачу при помощи метода наименьших квадратов. Напомним, что данный метод заключается в оптимизации функционала $MSE$:\n\n$$MSE(X, y) = \\frac{1}{l} \\sum_{i=1}^l (<w, x_i> - y_i)^2 \\to \\min_{w},$$\n\nгде $\\{ (x_i, y_i ) \\}_{i=1}^l$ — обучающая выборка, состоящая из $l$ пар объект-ответ.\n\nЗаметим, что решение данной задачи уже реализовано в модуле sklearn в виде класса [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression).\n\n**17. (0.5 балла)** Обучите линейную регрессию на 1000 объектах из обучающей выборки и выведите значения $MSE$ и $R^2$ на этой подвыборке и контрольной выборке (итого 4 различных числа). Проинтерпретируйте полученный результат — насколько качественные прогнозы строит полученная модель? Какие проблемы наблюдаются в модели?\n\n**Подсказка**: изучите значения полученных коэффициентов $w$, сохраненных в атрибуте coef_ объекта LinearRegression.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your code here",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Для решения описанных вами в предыдущем пункте проблем используем L1- или L2-регуляризацию, тем самым получив Lasso и Ridge регрессии соответственно и изменив оптимизационную задачу одним из следующих образов:\n$$MSE_{L1}(X, y) = \\frac{1}{l} \\sum_{i=1}^l (<w, x_i> - y_i)^2 + \\alpha ||w||_1 \\to \\min_{w},$$\n$$MSE_{L2}(X, y) = \\frac{1}{l} \\sum_{i=1}^l (<w, x_i> - y_i)^2 + \\alpha ||w||_2^2 \\to \\min_{w},$$\n\nгде $\\alpha$ — коэффициент регуляризации. Один из способов его подбора заключается в переборе некоторого количества значений и оценке качества на кросс-валидации для каждого из них, после чего выбирается значение, для которого было получено наилучшее качество.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Your code here",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "__18. (1 балл) __ Обучение линейной регрессии.\n\n\n\nОбучите линейную регрессию с $L_1$ (Lasso) и $L_2$ (Ridge) регуляризаторами (используйте параметры по умолчанию). Посмотрите, какое количество коэффициентов близко к 0 (степень близости к 0 определите сами из разумных пределов). Постройте график зависимости числа ненулевых коэффициентов от коэффицента регуляризации (перебирайте значения по логарифмической сетке от $10^{-3}$ до $10^3$). Согласуются ли результаты с вашими ожиданиями?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lasso = Lasso()\nlasso = lasso.fit(X_train_train, y_train_train)\nridge = Ridge()\nridge = ridge.fit(X_train_train, y_train_train)\n\nprint(\"0 Lasso:\", np.sum(np.absolute(lasso.coef_) < 10 ** (-7)))\nprint(\"0 Ridge:\", np.sum(np.absolute(ridge.coef_) < 10 ** (-7)))",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "l_zeros = []\nr_zeros = []\nalphas = 10 ** np.linspace(-3, 3, 20)\nfor alpha in alphas:\n    lasso = Lasso(alpha = alpha)\n    ridge = Ridge(alpha = alpha)\n    lasso.fit(X_train_train, y_train_train)\n    ridge.fit(X_train_train, y_train_train)\n    l_zeros.append(np.sum(np.absolute(lasso.coef_) < 10 ** (-7)))\n    r_zeros.append(np.sum(np.absolute(ridge.coef_) < 10 ** (-7)))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "fig, axs = plt.subplots(1, 2, figsize = (16, 7))\naxs[0].plot(alphas, l_zeros)\naxs[0].set_xlabel(\"Альфа\")\naxs[0].set_ylabel(\"Число с нулевыми коэф.\")\naxs[0].set_title(\"Лассо\")\naxs[0].set_xscale('log')\naxs[1].plot(alphas, r_zeros, color='red')\naxs[1].set_xlabel(\"Альфа\")\naxs[1].set_ylabel(\"Число с нулевыми коэф.\")\naxs[1].set_title(\"Ridge\")\naxs[1].set_xscale('log')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Посчитайте для Ridge-регрессии следующие метрики: $RMSE$, $MAE$, $R^2$.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def metrics(X_train, y_train, X_test, y_test, alpha = 1):\n    ridge = Ridge(alpha = alpha)\n    ridge.fit(X_train, y_train)\n    y_test_pred = ridge.predict(X_test)\n    print(\"RMSE:\", mean_squared_error(y_test, y_test_pred, squared = False))\n    print(\"MAE:\", mean_absolute_error(y_test, y_test_pred))\n    print(\"R^2:\", ridge.score(X_test, y_test))\nmetrics(X_train_train, y_train_train, X_test, y_test)",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "best_alpha = search_best_alpha('r2', X_train_train, y_train_train)\nmetrics(X_train_train, y_train_train, X_test, y_test, best_alpha)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Подберите на обучающей выборке для Ridge-регрессии коэффициент регуляризации (перебирайте значения по логарифмической сетке от $10^{-3}$ до $10^3$) для каждой из метрик при помощи кросс-валидации c 5 фолдами на тех же 1000 объектах. Для этого воспользуйтесь GridSearchCV и KFold из sklearn. Постройте графики зависимости фукнции потерь от коэффициента регуляризации. Посчитайте те же метрики снова. Заметно ли изменилось качество?\n\nДля выполнения данного задания вам могут понадобиться реализованные в библиотеке объекты [LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html), [RidgeCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) и [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\ndef search_best_alpha(scoring, X_train_train, y_train_train):\n    alphas = 10 ** np.linspace(-3, 3, 30)\n    searcher = GridSearchCV(Ridge(), [{\"alpha\": alphas}], scoring = scoring, cv = 5)\n    searcher.fit(X_train_train, y_train_train)\n    best_alpha = searcher.best_params_[\"alpha\"]\n    \n    print(f'Best a for {scoring} is, {best_alpha}')\n    plt.plot(alphas, -searcher.cv_results_[\"mean_test_score\"])\n    plt.xscale(\"log\")\n    plt.xlabel(\"alpha\")\n    plt.ylabel(\"CV score\")\n    plt.title(scoring)\n    return best_alpha",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "best_alpha = search_best_alpha('neg_root_mean_squared_error', X_train_train, y_train_train)\nmetrics(X_train_train, y_train_train, X_test, y_test, best_alpha)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "best_alpha = search_best_alpha('neg_mean_absolute_error', X_train_train, y_train_train)\nmetrics(X_train_train, y_train_train, X_test, y_test, best_alpha)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "__19. (0.5 балла)__ Поиск объектов-выбросов\n\n\nКак известно, MSE сильно штрафует за большие ошибки на объектах-выбросах. С помощью cross_val_predict сделайте Out-of-Fold предсказания для обучающей выборки. Посчитайте ошибки и посмотрите на их распределение (plt.hist). Что вы видите?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lasso = linear_model.Lasso(alpha=0.3162) \ny_pred = cross_val_predict(lasso, X_train, Y_train, cv=5) \nplt.hist(y_pred, bins=25)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "ridge = linear_model.Ridge(alpha=56.2341) \ny_pred = cross_val_predict(ridge, X_train, Y_train, cv=5) \nplt.hist(y_pred, bins=25)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}